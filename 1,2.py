# -*- coding: utf-8 -*-
"""resee_1,2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GQ6XyJZgCbqT73lUPLCOF8CukfBQeTbK

You can use this template for problems 1, 2 and 3
"""

import numpy as np
import time
# Students will submit their files with their team-name.py
# Student have to use the Team as their parent class

"""Recent Version of the Code"""

class Agent:
  def __init__(self):
      self.N = [0 for _ in range(6)]
      self.S = [0 for _ in range(6)]
      self.ucb = [1 for _ in range(6)]
      self.prev = -1
      self.t = 1

  def get_action(self,wicket,runs_scored):
    def kl_divergence(p, q):
      if p==q:
          return 0
      elif q == 0 or q == 1:
          return np.inf
      elif p == 0:
          return np.log(1/(1-q))
      elif p == 1:
          return np.log(1/q)
      return p * np.log(p/q) + (1-p) * np.log((1-p)/(1-q))

    def kl_confidence(t, emp_mean, num_pulls, precision = 1e-6):
      lower_bound = emp_mean
      upper_bound = 1
      while upper_bound - lower_bound > precision:
          q = (lower_bound + upper_bound) / 2
          if kl_divergence(emp_mean, q) > (np.log(t)/num_pulls):
              upper_bound = q
          else:
              lower_bound = q
      return (lower_bound + upper_bound)/2

    if self.prev != -1:
      self.N[self.prev] += 1
      self.t += 1
      self.S[self.prev] += runs_scored/6
      self.ucb[self.prev] = kl_confidence(self.t, ((self.S[self.prev]/self.N[self.prev])), self.N[self.prev])

    self.prev = np.argmax(self.ucb)
    return self.prev

class Environment:
  def __init__(self,num_balls,agent):
    self.num_balls = num_balls
    self.agent = agent
    self.__run_time = 0
    self.__total_runs = 0
    self.__total_wickets = 0
    self.__runs_scored = 0
    self.__start_time = 0
    self.__end_time = 0
    self.__regret_w = 0
    self.__regret_s = 0
    self.__wicket = 0
    self.__regret_rho = 0
    self.__p_out = np.random.rand(6)
    self.__p_run = np.array([1,0.9,0.85,0.8,0.75,0.7])
    self.__action_runs_map = np.array([0,1,2,3,4,6])
    self.__s = (1-self.__p_out)*self.__p_run*self.__action_runs_map
    self.__rho = self.__s/self.__p_out


  def __get_action(self):
    self.__start_time      = time. time()
    action          = self.agent.get_action(self.__wicket,self.__runs_scored)
    self.__end_time        = time. time()
    self.__run_time   = self.__run_time + self.__end_time - self.__start_time
    return action


  def __get_outcome(self, action):
    pout = self.__p_out[action]
    prun= self.__p_run[action]
    wicket = np.random.choice(2,1,p=[1-pout,pout])[0]
    runs = 0
    if(wicket==0):
      runs = self.__action_runs_map[action]*np.random.choice(2,1,p=[1-prun,prun])[0]
    return wicket, runs


  def innings(self):
    self.__total_runs = 0
    self.__total_wickets = 0
    self.__runs_scored = 0

    for ball in range(self.num_balls):
      action = self.__get_action()
      self.__wicket, self.__runs_scored   = self.__get_outcome(action)
      self.__total_runs     = self.__total_runs + self.__runs_scored
      self.__total_wickets  = self.__total_wickets + self.__wicket
      self.__regret_w       = self.__regret_w+ (self.__p_out[action]-np.min(self.__p_out))
      self.__regret_s       = self.__regret_s+ (np.max(self.__s) - self.__s[action])
      self.__regret_rho       = self.__regret_rho+ (np.max(self.__rho)-self.__rho[action])
    return self.__regret_w,self.__regret_s,self.__regret_rho, self.__total_runs, self.__total_wickets, self.__run_time

agent = Agent()
environment = Environment(100,agent)
regret_w,regret_s,reger_rho,total_runs,total_wickets,run_time = environment.innings()

print(regret_w,regret_s,reger_rho,total_runs,total_wickets,run_time)

balles = [10,100,1000,10000,100000]
regrer = []
regres = []
regrew = []
for balle in balles:
  agent = Agent()
  environment = Environment(balle,agent)
  regret_w,regret_s,reger_rho,total_runs,total_wickets,run_time = environment.innings()
  regrer.append(reger_rho)
  regres.append(regret_s)
  regrew.append(regret_w)

print(regrew)

print(regres)

print(regrer)

arr_s = np.zeros(100)
arr_w = np.zeros(100)
for num in range(100):
  agent = Agent()
  environment = Environment(100,agent)
  regret_w,regret_s,reger_rho,total_runs,total_wickets,run_time = environment.innings()
  arr_s[num] = regret_s
  arr_w[num] = regret_w

print(np.amin(arr_w),np.amax(arr_w),np.std(arr_w),np.mean(arr_w))

print(np.amin(arr_s),np.amax(arr_s),np.std(arr_s),np.mean(arr_s))

